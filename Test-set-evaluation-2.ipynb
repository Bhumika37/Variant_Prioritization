{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af5127b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b7e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Clinical-samples-test-data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6358b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = pd.read_excel(\"russel_lab_AA_db.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98646b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes on 'Main_AA' and 'Sub_AA' columns using a left join\n",
    "merged_df = pd.merge(df, file1, on=['Main_AA', 'Sub_AA'], how='left', suffixes=('_x', '_y'))\n",
    "\n",
    "def handle_unknown_aa(row):\n",
    "    # Ensure 'Unknown' is detected even if there are leading/trailing spaces\n",
    "    if 'Unknown' in str(row['Main_AA']).strip() or 'Unknown' in str(row['Sub_AA']).strip():\n",
    "        row['Substitution_Type'] = 'NA'\n",
    "        row['Score'] = 'NA'\n",
    "    return row\n",
    "\n",
    "# Apply the function to set Substitution_Type and Score to 'NA' if Sub_AA contains \"Unknown\"\n",
    "merged_df = merged_df.apply(handle_unknown_aa, axis=1)\n",
    "\n",
    "# Add the 'Substitution_Pref' column based on the prefix in 'Substitution_Type'\n",
    "def get_substitution_pref(sub_type):\n",
    "    if pd.isnull(sub_type) or sub_type == 'NA':\n",
    "        return 'NA'\n",
    "    if sub_type.startswith('APT'):\n",
    "        return 'All protein types'  # Only keep APT substitutions\n",
    "    return 'NA'  # Default to 'NA' for any non-APT type\n",
    "\n",
    "merged_df['Substitution_Pref'] = merged_df['Substitution_Type'].apply(get_substitution_pref)\n",
    "\n",
    "# Add the 'Substitution_Nature' column based on the suffix in 'Substitution_Type'\n",
    "def get_substitution_nature(sub_type):\n",
    "    if pd.isnull(sub_type) or sub_type == 'NA':\n",
    "        return 'NA'\n",
    "    if sub_type.endswith('DF'):\n",
    "        return 'Disfavoured'\n",
    "    elif sub_type.endswith('F'):\n",
    "        return 'Favoured'\n",
    "    elif sub_type.endswith('N'):\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "merged_df['Substitution_Nature'] = merged_df['Substitution_Type'].apply(get_substitution_nature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e34073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip extra spaces from the values in the 'clinvar: Clinvar' column\n",
    "merged_df['clinvar:_Clinvar'] = merged_df['clinvar:_Clinvar'].astype(str).str.strip()\n",
    "\n",
    "# Rearrangement of clinvar column\n",
    "# values to be replaced and the new value\n",
    "replace_dict = {\n",
    "    'clinvar: UNK': 'VUS',\n",
    "    'clinvar: other': 'VUS',\n",
    "    'clinvar: Conflicting_interpretations_of_pathogenicity': 'VUS',\n",
    "    'clinvar: Conflicting_interpretations_of_pathogenicity,_other': 'VUS',\n",
    "    'clinvar: Conflicting_interpretations_of_pathogenicity,_association': 'VUS',\n",
    "    'clinvar: Conflicting_interpretations_of_pathogenicity,_risk_factor': 'VUS',\n",
    "    'clinvar: Conflicting_interpretations_of_pathogenicity,_other,_risk_factor': 'VUS',\n",
    "    'clinvar: Conflicting_interpretations_of_pathogenicity,_association,_other,_risk_factor': 'VUS',\n",
    "    'clinvar: Uncertain_significance|drug_response': 'VUS',\n",
    "    'clinvar: Uncertain_significance': 'VUS',\n",
    "    'clinvar: Pathogenic/Likely_pathogenic': 'Pathogenic',\n",
    "    'clinvar: Pathogenic,_other,_risk_factor': 'Pathogenic',\n",
    "    'clinvar: Pathogenic/Likely_pathogenic,_risk_factor': 'Pathogenic',\n",
    "    'clinvar: Pathogenic,_risk_factor': 'Pathogenic',\n",
    "    'clinvar: Pathogenic/Likely_pathogenic,_other': 'Pathogenic',\n",
    "    'clinvar: drug_response': 'Pathogenic',\n",
    "    'clinvar: Pathogenic': 'Pathogenic',\n",
    "    'clinvar: Likely_pathogenic': 'Pathogenic',\n",
    "    'clinvar: Likely_pathogenic,_risk_factor': 'Pathogenic',\n",
    "    'clinvar: Pathogenic|drug_response|other': 'Pathogenic',\n",
    "    'clinvar: Pathogenic/Likely_pathogenic|drug_response': 'Pathogenic',\n",
    "    'clinvar: Pathogenic/Likely_pathogenic|other': 'Pathogenic',\n",
    "    'clinvar: Pathogenic/Likely_pathogenic|association|other': 'Pathogenic',\n",
    "    'clinvar: Likely_pathogenic|risk_factor': 'Pathogenic',\n",
    "    'clinvar: Benign': 'Benign',\n",
    "    'clinvar: Likely_benign': 'Benign',\n",
    "    'clinvar: not_provided': 'VUS',\n",
    "    'clinvar: Benign/Likely_benign': 'Benign',\n",
    "}\n",
    "\n",
    "# Replace the values in the 'clinvar' column\n",
    "merged_df['clinvar:_Clinvar'] = merged_df['clinvar:_Clinvar'].replace(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8307ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip extra spaces from the values in the 'clinvar: Clinvar' column\n",
    "merged_df['InterVar_automated'] = merged_df['InterVar_automated'].astype(str).str.strip()\n",
    "\n",
    "# Rearrangement of clinvar column\n",
    "# values to be replaced and the new value\n",
    "replace_dict = {\n",
    "'PATHOGENIC' : 'Pathogenic',\n",
    "'LIKELY_PATHOGENIC': 'Pathogenic',\n",
    "'LIKELY_BENIGN': 'Benign',\n",
    "'Likely_benign': 'Benign',\n",
    "'BENIGN': 'Benign',\n",
    "'UNCERTAIN_SIGNIFICANCE': 'VUS',\n",
    "'Uncertain_significance': 'VUS',\n",
    "'.': 'VUS'\n",
    "\n",
    "}\n",
    "\n",
    "# Replace the values in the 'clinvar' column\n",
    "merged_df['InterVar_automated'] = merged_df['InterVar_automated'].replace(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0c74d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Replace spaces in column names with underscores\n",
    "merged_df.columns = merged_df.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "520417b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where REF_x or ALT_x has a value of \".\"\n",
    "df = merged_df[(merged_df['REF_x'] != '.') & (merged_df['ALT_x'] != '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecccd11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sift = pd.get_dummies(merged_df.SIFT_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b4fee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_sift.rename(columns={\n",
    "    'D': 'SIFT_Deleterious',\n",
    "    'T': 'SIFT_Tolerated',\n",
    "    '.': 'SIFT_Unknown'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa2c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([merged_df,df_sift],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00a40dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polyphen = pd.get_dummies(merged_df.Polyphen2_HVAR_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c132fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_polyphen.rename(columns={\n",
    "    'D': 'Polyphen_Damaging',\n",
    "    'B': 'Polyphen_Benign',\n",
    "    '.': 'Polyphen_Unknown',\n",
    "    'P': 'Polyphen_Possibly_damaging'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6608a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([merged,df_polyphen],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3184833",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MT = pd.get_dummies(merged.MutationTaster_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0b48f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_MT.rename(columns={\n",
    "    'A': 'MutationTaster_Known_deleterious',\n",
    "    'N': 'MutationTaster_Probably_harmless',\n",
    "    'P': 'MutationTaster_Knowntobeharmless',\n",
    "    '.': 'MutationTaster_Unknown',\n",
    "    'D': 'MutationTaster_Probably_deleterious'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0ed77d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([merged,df_MT],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "977b896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final =  merged.drop(['SIFT_pred','Polyphen2_HVAR_pred','MutationTaster_pred'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70af4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin = pd.get_dummies(final['clinvar:_Clinvar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e4748b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "clin.rename(columns={\n",
    "    'Benign': 'Clinvar_Benign',\n",
    "    'Pathogenic': 'Clinvar_Pathogenic',\n",
    "    'VUS': 'Clinvar_VUS'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a58040a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged1 = pd.concat([final,clin],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e080df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervar = pd.get_dummies(merged1['InterVar_automated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9145fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "intervar.rename(columns={\n",
    "    'Benign': 'Intervar_Benign',\n",
    "    'Pathogenic': 'Intervar_Pathogenic',\n",
    "    'VUS': 'Intervar_VUS'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c80dce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged1 = pd.concat([merged1,intervar],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32ea8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 =  merged1.drop(['clinvar:_Clinvar', 'InterVar_automated'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdcaaae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_sub_pref = pd.get_dummies(final1['Substitution_Pref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e23ef151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Substitution_Pref_All protein types</th>\n",
       "      <th>Substitution_Pref_NA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Substitution_Pref_All protein types  Substitution_Pref_NA\n",
       "0                                  False                  True\n",
       "1                                  False                  True\n",
       "2                                  False                  True\n",
       "3                                  False                  True\n",
       "4                                  False                  True\n",
       "..                                   ...                   ...\n",
       "302                                 True                 False\n",
       "303                                False                  True\n",
       "304                                 True                 False\n",
       "305                                 True                 False\n",
       "306                                 True                 False\n",
       "\n",
       "[307 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_sub_pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a07f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rl_sub_pref.rename(columns={\n",
    "    'All protein types': 'Substitution_Pref_All protein types',\n",
    "    #'Extracellular proteins': 'RL_Extracellular_proteins',\n",
    "    #'Intracellular proteins': 'RL_Intracellular_proteins',\n",
    "    #'Membrane proteins': 'RL_Membrane_proteins',\n",
    "    'NA': 'Substitution_Pref_NA'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e1bf9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_RL1 = pd.concat([final1,rl_sub_pref],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2573971",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_RL1 =  merged_RL1.drop(['Substitution_Type', 'Score', 'Substitution_Pref'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5ce7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_sub_nature = pd.get_dummies(merged_RL1['Substitution_Nature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0528b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rl_sub_nature.rename(columns={\n",
    "    'Disfavoured': 'Substitution_Nature_Disfavoured',\n",
    "    'Favoured': 'Substitution_Nature_Favoured',\n",
    "    'NA': 'Substitution_Nature_NA',\n",
    "    'Neutral': 'Substitution_Nature_Neutral'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3584e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_RL2 = pd.concat([merged_RL1,rl_sub_nature],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "469606cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_RL2 =  merged_RL2.drop(['Substitution_Nature'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe1b6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the 'ExonicFunc.ensGene' column to lowercase\n",
    "merged_RL2['ExonicFunc.ensGene'] = merged_RL2['ExonicFunc.ensGene'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07e8ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: One-hot encoding for 'Ref.Gene' if it is a categorical column\n",
    "merged_RL2 = pd.get_dummies(merged_RL2, columns=['Func.ensGene', 'ExonicFunc.ensGene','Ref.Gene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df642a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_RL2 = merged_RL2.drop(['CHROM_x','POS_x', 'End_x', 'REF_x','ALT_x','AA_Change'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac2e8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: One-hot encoding for 'Ref.Gene' if it is a categorical column\n",
    "merged_RL2 = pd.get_dummies(merged_RL2, columns=['Main_AA', 'Sub_AA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63cad4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean columns to integer (0, 1)\n",
    "bool_cols = merged_RL2.select_dtypes(include='bool').columns\n",
    "merged_RL2[bool_cols] = merged_RL2[bool_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2caaebd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT_Unknown: int32\n",
      "SIFT_Deleterious: int32\n",
      "SIFT_Tolerated: int32\n",
      "Polyphen_Unknown: int32\n",
      "Polyphen_Benign: int32\n",
      "Polyphen_Damaging: int32\n",
      "Polyphen_Possibly_damaging: int32\n",
      "MutationTaster_Unknown: int32\n",
      "MutationTaster_Known_deleterious: int32\n",
      "MutationTaster_Probably_deleterious: int32\n",
      "MutationTaster_Probably_harmless: int32\n",
      "Clinvar_Benign: int32\n",
      "Clinvar_Pathogenic: int32\n",
      "Clinvar_VUS: int32\n",
      "Intervar_Benign: int32\n",
      "Intervar_Pathogenic: int32\n",
      "Intervar_VUS: int32\n",
      "Substitution_Pref_All protein types: int32\n",
      "Substitution_Pref_NA: int32\n",
      "Substitution_Nature_Disfavoured: int32\n",
      "Substitution_Nature_Favoured: int32\n",
      "Substitution_Nature_NA: int32\n",
      "Substitution_Nature_Neutral: int32\n",
      "Func.ensGene_exonic: int32\n",
      "Func.ensGene_splicing: int32\n",
      "ExonicFunc.ensGene_.: int32\n",
      "ExonicFunc.ensGene_frameshift deletion: int32\n",
      "ExonicFunc.ensGene_frameshift insertion: int32\n",
      "ExonicFunc.ensGene_nonframeshift deletion: int32\n",
      "ExonicFunc.ensGene_nonframeshift insertion: int32\n",
      "ExonicFunc.ensGene_nonsynonymous snv: int32\n",
      "ExonicFunc.ensGene_stopgain: int32\n",
      "ExonicFunc.ensGene_unknown: int32\n",
      "Ref.Gene_ALK: int32\n",
      "Ref.Gene_APC: int32\n",
      "Ref.Gene_AR: int32\n",
      "Ref.Gene_ATM: int32\n",
      "Ref.Gene_BARD1: int32\n",
      "Ref.Gene_BMPR1A: int32\n",
      "Ref.Gene_BRAF: int32\n",
      "Ref.Gene_BRCA1: int32\n",
      "Ref.Gene_BRCA2: int32\n",
      "Ref.Gene_BRIP1: int32\n",
      "Ref.Gene_CDK12: int32\n",
      "Ref.Gene_CDKN2A: int32\n",
      "Ref.Gene_CHEK1: int32\n",
      "Ref.Gene_CHEK2: int32\n",
      "Ref.Gene_CTNNB1: int32\n",
      "Ref.Gene_EGFR: int32\n",
      "Ref.Gene_ERBB2: int32\n",
      "Ref.Gene_FANCL: int32\n",
      "Ref.Gene_FGFR1: int32\n",
      "Ref.Gene_FGFR2: int32\n",
      "Ref.Gene_FGFR3: int32\n",
      "Ref.Gene_GAPDH: int32\n",
      "Ref.Gene_IDH2: int32\n",
      "Ref.Gene_JAK2: int32\n",
      "Ref.Gene_KIT: int32\n",
      "Ref.Gene_KRAS: int32\n",
      "Ref.Gene_MAP2K2: int32\n",
      "Ref.Gene_MDM2: int32\n",
      "Ref.Gene_MET: int32\n",
      "Ref.Gene_MLH1: int32\n",
      "Ref.Gene_MLH3: int32\n",
      "Ref.Gene_MSH2: int32\n",
      "Ref.Gene_MSH6: int32\n",
      "Ref.Gene_MUTYH: int32\n",
      "Ref.Gene_NTRK1: int32\n",
      "Ref.Gene_PALB2: int32\n",
      "Ref.Gene_PDGFRA: int32\n",
      "Ref.Gene_PIK3CA: int32\n",
      "Ref.Gene_PMS1: int32\n",
      "Ref.Gene_PMS2: int32\n",
      "Ref.Gene_POLD1: int32\n",
      "Ref.Gene_POLE: int32\n",
      "Ref.Gene_POLH: int32\n",
      "Ref.Gene_PTEN: int32\n",
      "Ref.Gene_RAD50: int32\n",
      "Ref.Gene_RAD51: int32\n",
      "Ref.Gene_RAD51B: int32\n",
      "Ref.Gene_RAD54L: int32\n",
      "Ref.Gene_RB1: int32\n",
      "Ref.Gene_RET: int32\n",
      "Ref.Gene_ROS1: int32\n",
      "Ref.Gene_STK11: int32\n",
      "Ref.Gene_TP53: int32\n",
      "Ref.Gene_TSC1: int32\n",
      "Ref.Gene_TSC2: int32\n",
      "Main_AA_Ala: int32\n",
      "Main_AA_Arg: int32\n",
      "Main_AA_Asn: int32\n",
      "Main_AA_Asp: int32\n",
      "Main_AA_Cys: int32\n",
      "Main_AA_Del_Unknown: int32\n",
      "Main_AA_Gln: int32\n",
      "Main_AA_Glu: int32\n",
      "Main_AA_Gly: int32\n",
      "Main_AA_His: int32\n",
      "Main_AA_Ile: int32\n",
      "Main_AA_Leu: int32\n",
      "Main_AA_Lys: int32\n",
      "Main_AA_Met: int32\n",
      "Main_AA_Phe: int32\n",
      "Main_AA_Pro: int32\n",
      "Main_AA_Ser: int32\n",
      "Main_AA_Thr: int32\n",
      "Main_AA_Trp: int32\n",
      "Main_AA_Tyr: int32\n",
      "Main_AA_Unknown: int32\n",
      "Main_AA_Val: int32\n",
      "Sub_AA_ALa: int32\n",
      "Sub_AA_Ala: int32\n",
      "Sub_AA_Arg: int32\n",
      "Sub_AA_Asn: int32\n",
      "Sub_AA_Asp: int32\n",
      "Sub_AA_Cys: int32\n",
      "Sub_AA_Del_Unknown: int32\n",
      "Sub_AA_Gln: int32\n",
      "Sub_AA_Glu: int32\n",
      "Sub_AA_Gly: int32\n",
      "Sub_AA_His: int32\n",
      "Sub_AA_Ile: int32\n",
      "Sub_AA_Leu: int32\n",
      "Sub_AA_Lys: int32\n",
      "Sub_AA_Met: int32\n",
      "Sub_AA_Phe: int32\n",
      "Sub_AA_Pro: int32\n",
      "Sub_AA_Ser: int32\n",
      "Sub_AA_Ter_Unknown: int32\n",
      "Sub_AA_Thr: int32\n",
      "Sub_AA_Trp: int32\n",
      "Sub_AA_Tyr: int32\n",
      "Sub_AA_Unknown: int32\n",
      "Sub_AA_Val: int32\n",
      "Sub_AA_delins_Unknown: int32\n",
      "Sub_AA_fs_Unknown: int32\n",
      "MutationTaster_Knowntobeharmless: int64\n",
      "RL_All_Protein_types: int64\n",
      "RL_Disfavoured: int64\n",
      "RL_Favoured: int64\n",
      "NA_RL: int64\n",
      "RL_Neutral: int64\n",
      "Func.ensGene_UTR3: int64\n",
      "Func.ensGene_UTR5: int64\n",
      "Func.ensGene_exonic;splicing: int64\n",
      "Func.ensGene_intronic: int64\n",
      "Func.ensGene_ncRNA_intronic: int64\n",
      "ExonicFunc.ensGene_nonframeshift substitution: int64\n",
      "ExonicFunc.ensGene_stoploss: int64\n",
      "ExonicFunc.ensGene_synonymous snv: int64\n"
     ]
    }
   ],
   "source": [
    "# Print all column names with their data types\n",
    "for col in merged_RL2.columns:\n",
    "    print(f\"{col}: {merged_RL2[col].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b3f171c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SIFT_Unknown',\n",
       " 'SIFT_Deleterious',\n",
       " 'SIFT_Tolerated',\n",
       " 'Polyphen_Unknown',\n",
       " 'Polyphen_Benign',\n",
       " 'Polyphen_Damaging',\n",
       " 'Polyphen_Possibly_damaging',\n",
       " 'MutationTaster_Unknown',\n",
       " 'MutationTaster_Known_deleterious',\n",
       " 'MutationTaster_Probably_deleterious',\n",
       " 'MutationTaster_Probably_harmless',\n",
       " 'MutationTaster_Knowntobeharmless',\n",
       " 'Clinvar_Benign',\n",
       " 'Clinvar_Pathogenic',\n",
       " 'Clinvar_VUS',\n",
       " 'Intervar_Benign',\n",
       " 'Intervar_Pathogenic',\n",
       " 'Intervar_VUS',\n",
       " 'RL_All_Protein_types',\n",
       " 'RL_Disfavoured',\n",
       " 'RL_Favoured',\n",
       " 'NA_RL',\n",
       " 'RL_Neutral',\n",
       " 'Func.ensGene_UTR3',\n",
       " 'Func.ensGene_UTR5',\n",
       " 'Func.ensGene_exonic',\n",
       " 'Func.ensGene_exonic;splicing',\n",
       " 'Func.ensGene_intronic',\n",
       " 'Func.ensGene_ncRNA_intronic',\n",
       " 'Func.ensGene_splicing',\n",
       " 'ExonicFunc.ensGene_.',\n",
       " 'ExonicFunc.ensGene_frameshift deletion',\n",
       " 'ExonicFunc.ensGene_frameshift insertion',\n",
       " 'ExonicFunc.ensGene_nonframeshift deletion',\n",
       " 'ExonicFunc.ensGene_nonframeshift insertion',\n",
       " 'ExonicFunc.ensGene_nonframeshift substitution',\n",
       " 'ExonicFunc.ensGene_nonsynonymous snv',\n",
       " 'ExonicFunc.ensGene_stopgain',\n",
       " 'ExonicFunc.ensGene_stoploss',\n",
       " 'ExonicFunc.ensGene_synonymous snv',\n",
       " 'ExonicFunc.ensGene_unknown',\n",
       " 'Main_AA_Ala',\n",
       " 'Main_AA_Arg',\n",
       " 'Main_AA_Asn',\n",
       " 'Main_AA_Asp',\n",
       " 'Main_AA_Cys',\n",
       " 'Main_AA_Del_Unknown',\n",
       " 'Main_AA_Gln',\n",
       " 'Main_AA_Glu',\n",
       " 'Main_AA_Gly',\n",
       " 'Main_AA_His',\n",
       " 'Main_AA_Ile',\n",
       " 'Main_AA_Leu',\n",
       " 'Main_AA_Lys',\n",
       " 'Main_AA_Met',\n",
       " 'Main_AA_Phe',\n",
       " 'Main_AA_Pro',\n",
       " 'Main_AA_Ser',\n",
       " 'Main_AA_Thr',\n",
       " 'Main_AA_Trp',\n",
       " 'Main_AA_Tyr',\n",
       " 'Main_AA_Unknown',\n",
       " 'Main_AA_Val',\n",
       " 'Sub_AA_Ala',\n",
       " 'Sub_AA_Arg',\n",
       " 'Sub_AA_Asn',\n",
       " 'Sub_AA_Asp',\n",
       " 'Sub_AA_Cys',\n",
       " 'Sub_AA_Del_Unknown',\n",
       " 'Sub_AA_Gln',\n",
       " 'Sub_AA_Glu',\n",
       " 'Sub_AA_Gly',\n",
       " 'Sub_AA_His',\n",
       " 'Sub_AA_Ile',\n",
       " 'Sub_AA_Leu',\n",
       " 'Sub_AA_Lys',\n",
       " 'Sub_AA_Met',\n",
       " 'Sub_AA_Phe',\n",
       " 'Sub_AA_Pro',\n",
       " 'Sub_AA_Ser',\n",
       " 'Sub_AA_Ter_Unknown',\n",
       " 'Sub_AA_Thr',\n",
       " 'Sub_AA_Trp',\n",
       " 'Sub_AA_Tyr',\n",
       " 'Sub_AA_Unknown',\n",
       " 'Sub_AA_Val',\n",
       " 'Sub_AA_delins_Unknown',\n",
       " 'Sub_AA_fs_Unknown']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f6e497c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: ['MutationTaster_Knowntobeharmless', 'Func.ensGene_UTR3', 'Func.ensGene_UTR5', 'Func.ensGene_exonic;splicing', 'Func.ensGene_intronic', 'Func.ensGene_ncRNA_intronic', 'ExonicFunc.ensGene_nonframeshift substitution', 'ExonicFunc.ensGene_stoploss', 'ExonicFunc.ensGene_synonymous snv']\n",
      "Extra columns: ['RL_NA', 'Ref.Gene_ALK', 'Ref.Gene_APC', 'Ref.Gene_AR', 'Ref.Gene_ATM', 'Ref.Gene_BARD1', 'Ref.Gene_BMPR1A', 'Ref.Gene_BRAF', 'Ref.Gene_BRCA1', 'Ref.Gene_BRCA2', 'Ref.Gene_BRIP1', 'Ref.Gene_CDK12', 'Ref.Gene_CDKN2A', 'Ref.Gene_CHEK1', 'Ref.Gene_CHEK2', 'Ref.Gene_CTNNB1', 'Ref.Gene_EGFR', 'Ref.Gene_ERBB2', 'Ref.Gene_FANCL', 'Ref.Gene_FGFR1', 'Ref.Gene_FGFR2', 'Ref.Gene_FGFR3', 'Ref.Gene_GAPDH', 'Ref.Gene_IDH2', 'Ref.Gene_JAK2', 'Ref.Gene_KIT', 'Ref.Gene_KRAS', 'Ref.Gene_MAP2K2', 'Ref.Gene_MDM2', 'Ref.Gene_MET', 'Ref.Gene_MLH1', 'Ref.Gene_MLH3', 'Ref.Gene_MSH2', 'Ref.Gene_MSH6', 'Ref.Gene_MUTYH', 'Ref.Gene_NTRK1', 'Ref.Gene_PALB2', 'Ref.Gene_PDGFRA', 'Ref.Gene_PIK3CA', 'Ref.Gene_PMS1', 'Ref.Gene_PMS2', 'Ref.Gene_POLD1', 'Ref.Gene_POLE', 'Ref.Gene_POLH', 'Ref.Gene_PTEN', 'Ref.Gene_RAD50', 'Ref.Gene_RAD51', 'Ref.Gene_RAD51B', 'Ref.Gene_RAD54L', 'Ref.Gene_RB1', 'Ref.Gene_RET', 'Ref.Gene_ROS1', 'Ref.Gene_STK11', 'Ref.Gene_TP53', 'Ref.Gene_TSC1', 'Ref.Gene_TSC2', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       'MutationTaster_Probably_harmless', 'MutationTaster_Knowntobeharmless',\n",
      "       'Clinvar_Benign', 'Clinvar_Pathogenic', 'Clinvar_VUS',\n",
      "       'Intervar_Benign', 'Intervar_Pathogenic', 'Intervar_VUS',\n",
      "       'RL_All_Protein_types', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL',\n",
      "       'RL_Neutral', 'Func.ensGene_UTR3', 'Func.ensGene_UTR5',\n",
      "       'Func.ensGene_exonic', 'Func.ensGene_exonic;splicing',\n",
      "       'Func.ensGene_intronic', 'Func.ensGene_ncRNA_intronic',\n",
      "       'Func.ensGene_splicing', 'ExonicFunc.ensGene_.',\n",
      "       'ExonicFunc.ensGene_frameshift deletion',\n",
      "       'ExonicFunc.ensGene_frameshift insertion',\n",
      "       'ExonicFunc.ensGene_nonframeshift deletion',\n",
      "       'ExonicFunc.ensGene_nonframeshift insertion',\n",
      "       'ExonicFunc.ensGene_nonframeshift substitution',\n",
      "       'ExonicFunc.ensGene_nonsynonymous snv', 'ExonicFunc.ensGene_stopgain',\n",
      "       'ExonicFunc.ensGene_stoploss', 'ExonicFunc.ensGene_synonymous snv',\n",
      "       'ExonicFunc.ensGene_unknown', 'Main_AA_Ala', 'Main_AA_Arg',\n",
      "       'Main_AA_Asn', 'Main_AA_Asp', 'Main_AA_Cys', 'Main_AA_Del_Unknown',\n",
      "       'Main_AA_Gln', 'Main_AA_Glu', 'Main_AA_Gly', 'Main_AA_His',\n",
      "       'Main_AA_Ile', 'Main_AA_Leu', 'Main_AA_Lys', 'Main_AA_Met',\n",
      "       'Main_AA_Phe', 'Main_AA_Pro', 'Main_AA_Ser', 'Main_AA_Thr',\n",
      "       'Main_AA_Trp', 'Main_AA_Tyr', 'Main_AA_Unknown', 'Main_AA_Val',\n",
      "       'Sub_AA_Ala', 'Sub_AA_Arg', 'Sub_AA_Asn', 'Sub_AA_Asp', 'Sub_AA_Cys',\n",
      "       'Sub_AA_Del_Unknown', 'Sub_AA_Gln', 'Sub_AA_Glu', 'Sub_AA_Gly',\n",
      "       'Sub_AA_His', 'Sub_AA_Ile', 'Sub_AA_Leu', 'Sub_AA_Lys', 'Sub_AA_Met',\n",
      "       'Sub_AA_Phe', 'Sub_AA_Pro', 'Sub_AA_Ser', 'Sub_AA_Ter_Unknown',\n",
      "       'Sub_AA_Thr', 'Sub_AA_Trp', 'Sub_AA_Tyr', 'Sub_AA_Unknown',\n",
      "       'Sub_AA_Val', 'Sub_AA_delins_Unknown', 'Sub_AA_fs_Unknown'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_19420\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_19420\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_19420\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_19420\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_19420\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_19420\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_19420\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_19420\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_19420\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_LR = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/Target-2/LR_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/Target-1/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_LR.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = model_LR.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "020c9001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: ['MutationTaster_Knowntobeharmless', 'RL_All_Protein_types', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Func.ensGene_UTR3', 'Func.ensGene_UTR5', 'Func.ensGene_exonic;splicing', 'Func.ensGene_intronic', 'Func.ensGene_ncRNA_intronic', 'ExonicFunc.ensGene_nonframeshift substitution', 'ExonicFunc.ensGene_stoploss', 'ExonicFunc.ensGene_synonymous snv']\n",
      "Extra columns: ['Substitution_Pref_All protein types', 'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured', 'Substitution_Nature_Favoured', 'Substitution_Nature_NA', 'Substitution_Nature_Neutral', 'Ref.Gene_ALK', 'Ref.Gene_APC', 'Ref.Gene_AR', 'Ref.Gene_ATM', 'Ref.Gene_BARD1', 'Ref.Gene_BMPR1A', 'Ref.Gene_BRAF', 'Ref.Gene_BRCA1', 'Ref.Gene_BRCA2', 'Ref.Gene_BRIP1', 'Ref.Gene_CDK12', 'Ref.Gene_CDKN2A', 'Ref.Gene_CHEK1', 'Ref.Gene_CHEK2', 'Ref.Gene_CTNNB1', 'Ref.Gene_EGFR', 'Ref.Gene_ERBB2', 'Ref.Gene_FANCL', 'Ref.Gene_FGFR1', 'Ref.Gene_FGFR2', 'Ref.Gene_FGFR3', 'Ref.Gene_GAPDH', 'Ref.Gene_IDH2', 'Ref.Gene_JAK2', 'Ref.Gene_KIT', 'Ref.Gene_KRAS', 'Ref.Gene_MAP2K2', 'Ref.Gene_MDM2', 'Ref.Gene_MET', 'Ref.Gene_MLH1', 'Ref.Gene_MLH3', 'Ref.Gene_MSH2', 'Ref.Gene_MSH6', 'Ref.Gene_MUTYH', 'Ref.Gene_NTRK1', 'Ref.Gene_PALB2', 'Ref.Gene_PDGFRA', 'Ref.Gene_PIK3CA', 'Ref.Gene_PMS1', 'Ref.Gene_PMS2', 'Ref.Gene_POLD1', 'Ref.Gene_POLE', 'Ref.Gene_POLH', 'Ref.Gene_PTEN', 'Ref.Gene_RAD50', 'Ref.Gene_RAD51', 'Ref.Gene_RAD51B', 'Ref.Gene_RAD54L', 'Ref.Gene_RB1', 'Ref.Gene_RET', 'Ref.Gene_ROS1', 'Ref.Gene_STK11', 'Ref.Gene_TP53', 'Ref.Gene_TSC1', 'Ref.Gene_TSC2', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       'MutationTaster_Probably_harmless', 'MutationTaster_Knowntobeharmless',\n",
      "       'Clinvar_Benign', 'Clinvar_Pathogenic', 'Clinvar_VUS',\n",
      "       'Intervar_Benign', 'Intervar_Pathogenic', 'Intervar_VUS',\n",
      "       'RL_All_Protein_types', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL',\n",
      "       'RL_Neutral', 'Func.ensGene_UTR3', 'Func.ensGene_UTR5',\n",
      "       'Func.ensGene_exonic', 'Func.ensGene_exonic;splicing',\n",
      "       'Func.ensGene_intronic', 'Func.ensGene_ncRNA_intronic',\n",
      "       'Func.ensGene_splicing', 'ExonicFunc.ensGene_.',\n",
      "       'ExonicFunc.ensGene_frameshift deletion',\n",
      "       'ExonicFunc.ensGene_frameshift insertion',\n",
      "       'ExonicFunc.ensGene_nonframeshift deletion',\n",
      "       'ExonicFunc.ensGene_nonframeshift insertion',\n",
      "       'ExonicFunc.ensGene_nonframeshift substitution',\n",
      "       'ExonicFunc.ensGene_nonsynonymous snv', 'ExonicFunc.ensGene_stopgain',\n",
      "       'ExonicFunc.ensGene_stoploss', 'ExonicFunc.ensGene_synonymous snv',\n",
      "       'ExonicFunc.ensGene_unknown', 'Main_AA_Ala', 'Main_AA_Arg',\n",
      "       'Main_AA_Asn', 'Main_AA_Asp', 'Main_AA_Cys', 'Main_AA_Del_Unknown',\n",
      "       'Main_AA_Gln', 'Main_AA_Glu', 'Main_AA_Gly', 'Main_AA_His',\n",
      "       'Main_AA_Ile', 'Main_AA_Leu', 'Main_AA_Lys', 'Main_AA_Met',\n",
      "       'Main_AA_Phe', 'Main_AA_Pro', 'Main_AA_Ser', 'Main_AA_Thr',\n",
      "       'Main_AA_Trp', 'Main_AA_Tyr', 'Main_AA_Unknown', 'Main_AA_Val',\n",
      "       'Sub_AA_Ala', 'Sub_AA_Arg', 'Sub_AA_Asn', 'Sub_AA_Asp', 'Sub_AA_Cys',\n",
      "       'Sub_AA_Del_Unknown', 'Sub_AA_Gln', 'Sub_AA_Glu', 'Sub_AA_Gly',\n",
      "       'Sub_AA_His', 'Sub_AA_Ile', 'Sub_AA_Leu', 'Sub_AA_Lys', 'Sub_AA_Met',\n",
      "       'Sub_AA_Phe', 'Sub_AA_Pro', 'Sub_AA_Ser', 'Sub_AA_Ter_Unknown',\n",
      "       'Sub_AA_Thr', 'Sub_AA_Trp', 'Sub_AA_Tyr', 'Sub_AA_Unknown',\n",
      "       'Sub_AA_Val', 'Sub_AA_delins_Unknown', 'Sub_AA_fs_Unknown'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7564\\2219701987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_LR = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/Target-2/LR_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/Target-1/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_LR.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = model_LR.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5db314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: If predictions are one-hot encoded, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the predicted class (highest probability)\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # pred is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_LR.xlsx\", index=False) \n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78547e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\2654291728.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\2654291728.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.999741\n",
      "1          Benign               0.908803\n",
      "2          Benign               0.956938\n",
      "3             VUS               0.647669\n",
      "4          Benign               0.998262\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_LR_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/LR_model_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_LR_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = model_LR_smote.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are one-hot encoded, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the predicted class (highest probability)\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # pred is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_LR_smote.xlsx\", index=False) \n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b327da06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n",
      "Predictions shape: (307, 3)\n",
      "Predictions sample: [[0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\3331699896.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\3331699896.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic                    1.0\n",
      "1          Benign                    1.0\n",
      "2          Benign                    1.0\n",
      "3             VUS                    1.0\n",
      "4          Benign                    1.0\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_KNN = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/KNN_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_KNN.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = model_KNN.predict(test_data)\n",
    "\n",
    "# Check the shape of predictions to see if it's a one-hot encoded array\n",
    "print(\"Predictions shape:\", predictions.shape)  # Debugging line\n",
    "print(\"Predictions sample:\", predictions[:5])  # Debugging line\n",
    "\n",
    "# Step 3: If predictions are one-hot encoded, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the predicted class (highest probability)\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_KNN.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ffad988e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n",
      "Predictions shape: (307, 3)\n",
      "Predictions sample: [[0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\2159294720.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\2159294720.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic                    1.0\n",
      "1          Benign                    1.0\n",
      "2          Benign                    1.0\n",
      "3             VUS                    1.0\n",
      "4          Benign                    1.0\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_KNN_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/KNN_model_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_KNN_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = model_KNN_smote.predict(test_data)\n",
    "\n",
    "# Check the shape of predictions to see if it's a one-hot encoded array\n",
    "print(\"Predictions shape:\", predictions.shape)  # Debugging line\n",
    "print(\"Predictions sample:\", predictions[:5])  # Debugging line\n",
    "\n",
    "# Step 3: If predictions are one-hot encoded, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the predicted class (highest probability)\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_KNN_smote_prob.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb0bfcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n",
      "Predictions shape: (307, 3)\n",
      "Predictions sample: [[0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\1610554578.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\1610554578.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic                    1.0\n",
      "1          Benign                    1.0\n",
      "2          Benign                    1.0\n",
      "3          Benign                    1.0\n",
      "4          Benign                    1.0\n"
     ]
    }
   ],
   "source": [
    "model_DT = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/DT_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_DT.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (class labels directly)\n",
    "predictions = model_DT.predict(test_data)\n",
    "\n",
    "# Check the shape of predictions to see if it's a one-hot encoded array\n",
    "print(\"Predictions shape:\", predictions.shape)  # Debugging line\n",
    "print(\"Predictions sample:\", predictions[:5])  # Debugging line\n",
    "\n",
    "# Step 3: If predictions are one-hot encoded, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the predicted class (highest probability)\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_DT.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ea95372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n",
      "Predictions shape: (307, 3)\n",
      "Predictions sample: [[0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\1199173306.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\1199173306.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic                    1.0\n",
      "1          Benign                    1.0\n",
      "2          Benign                    1.0\n",
      "3          Benign                    1.0\n",
      "4          Benign                    1.0\n"
     ]
    }
   ],
   "source": [
    "model_DT_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/DT_model_with_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_DT_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (class labels directly)\n",
    "predictions = model_DT_smote.predict(test_data)\n",
    "\n",
    "# Check the shape of predictions to see if it's a one-hot encoded array\n",
    "print(\"Predictions shape:\", predictions.shape)  # Debugging line\n",
    "print(\"Predictions sample:\", predictions[:5])  # Debugging line\n",
    "\n",
    "# Step 3: If predictions are one-hot encoded, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the predicted class (highest probability)\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_DT_smote_prob.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42e83b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\2714831673.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\2714831673.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.880000\n",
      "1          Benign               1.000000\n",
      "2          Benign               0.756000\n",
      "3             VUS               0.636667\n",
      "4          Benign               1.000000\n"
     ]
    }
   ],
   "source": [
    "model_RF = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/RF_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_RF.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (class labels directly)\n",
    "predictions = model_RF.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a one-hot encoded format, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_RF.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3572c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\1750489467.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\1750489467.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic                   0.96\n",
      "1          Benign                   0.70\n",
      "2          Benign                   0.44\n",
      "3             VUS                   0.95\n",
      "4          Benign                   0.78\n"
     ]
    }
   ],
   "source": [
    "model_RF_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/RF_model_with_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_RF_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (class labels directly)\n",
    "predictions = model_RF_smote.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a one-hot encoded format, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_RF_smote_prob.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99622248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\2140304260.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\2140304260.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.981742\n",
      "1          Benign               0.993754\n",
      "2          Benign               0.995632\n",
      "3             VUS               0.829984\n",
      "4          Benign               0.999189\n"
     ]
    }
   ],
   "source": [
    "XGB_model = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/XGB_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = XGB_model.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (class labels directly)\n",
    "predictions = XGB_model.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a one-hot encoded format, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_XGB.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "174456b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\4073786871.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\4073786871.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.994192\n",
      "1          Benign               0.857237\n",
      "2          Benign               0.974760\n",
      "3             VUS               0.963470\n",
      "4          Benign               0.973427\n"
     ]
    }
   ],
   "source": [
    "XGB_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/XGB_with_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = XGB_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (class labels directly)\n",
    "predictions = XGB_smote.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a one-hot encoded format, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_XGB_smote_prob.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e4c13fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\2396374115.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\2396374115.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.984495\n",
      "1          Benign               0.995016\n",
      "2          Benign               0.996869\n",
      "3             VUS               0.874930\n",
      "4          Benign               0.996667\n"
     ]
    }
   ],
   "source": [
    "LGB_model = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/LGB_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = LGB_model.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = LGB_model.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a 2D array (one-hot encoded), use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_LGB.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d757737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\961185419.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\961185419.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.995881\n",
      "1          Benign               0.939353\n",
      "2          Benign               0.978145\n",
      "3             VUS               0.901472\n",
      "4          Benign               0.991896\n"
     ]
    }
   ],
   "source": [
    "LGB_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/LGB_with_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = LGB_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = LGB_smote.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a 2D array (one-hot encoded), use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_LGB_smote_prob.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d78587b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\2860687054.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\2860687054.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.970503\n",
      "1          Benign               0.993654\n",
      "2          Benign               0.977910\n",
      "3          Benign               0.202583\n",
      "4          Benign               0.998388\n"
     ]
    }
   ],
   "source": [
    "CB_model = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/CB_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained CatBoost model\n",
    "pred_probabilities = CB_model.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with the highest probability)\n",
    "predictions = CB_model.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a 2D array (one-hot encoded), use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_CB.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e803b207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['RL_All_Protein_types', 'RL_NA', 'RL_Disfavoured', 'RL_Favoured', 'NA_RL', 'RL_Neutral', 'Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All protein types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\4234207993.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15648\\4234207993.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.999884\n",
      "1          Benign               0.723974\n",
      "2          Benign               0.807591\n",
      "3             VUS               0.969499\n",
      "4          Benign               0.986065\n"
     ]
    }
   ],
   "source": [
    "CB_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/CB_with_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained CatBoost model\n",
    "pred_probabilities = CB_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with the highest probability)\n",
    "predictions = CB_smote.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a 2D array (one-hot encoded), use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_CAT_smote_prob.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d510812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
