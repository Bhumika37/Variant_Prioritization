{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af5127b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b7e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Clinical-samples-test-data-2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6358b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = pd.read_excel(\"russel_lab_AA_db.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98646b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes on 'Main_AA' and 'Sub_AA' columns using a left join\n",
    "merged_df = pd.merge(df, file1, on=['Main_AA', 'Sub_AA'], how='left', suffixes=('_x', '_y'))\n",
    "\n",
    "def handle_unknown_aa(row):\n",
    "    # Ensure 'Unknown' is detected even if there are leading/trailing spaces\n",
    "    if 'Unknown' in str(row['Main_AA']).strip() or 'Unknown' in str(row['Sub_AA']).strip():\n",
    "        row['Substitution_Type'] = 'NA'\n",
    "        row['Score'] = 'NA'\n",
    "    return row\n",
    "\n",
    "# Apply the function to set Substitution_Type and Score to 'NA' if Sub_AA contains \"Unknown\"\n",
    "merged_df = merged_df.apply(handle_unknown_aa, axis=1)\n",
    "\n",
    "# Add the 'Substitution_Pref' column based on the prefix in 'Substitution_Type'\n",
    "def get_substitution_pref(sub_type):\n",
    "    if pd.isnull(sub_type) or sub_type == 'NA':\n",
    "        return 'NA'\n",
    "    if sub_type.startswith('APT'):\n",
    "        return 'All protein types'  # Only keep APT substitutions\n",
    "    return 'NA'  # Default to 'NA' for any non-APT type\n",
    "\n",
    "merged_df['Substitution_Pref'] = merged_df['Substitution_Type'].apply(get_substitution_pref)\n",
    "\n",
    "# Add the 'Substitution_Nature' column based on the suffix in 'Substitution_Type'\n",
    "def get_substitution_nature(sub_type):\n",
    "    if pd.isnull(sub_type) or sub_type == 'NA':\n",
    "        return 'NA'\n",
    "    if sub_type.endswith('DF'):\n",
    "        return 'Disfavoured'\n",
    "    elif sub_type.endswith('F'):\n",
    "        return 'Favoured'\n",
    "    elif sub_type.endswith('N'):\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "merged_df['Substitution_Nature'] = merged_df['Substitution_Type'].apply(get_substitution_nature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e34073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip extra spaces from the values in the 'clinvar: Clinvar' column\n",
    "merged_df['clinvar:_Clinvar'] = merged_df['clinvar:_Clinvar'].astype(str).str.strip()\n",
    "\n",
    "# Rearrangement of clinvar column\n",
    "# values to be replaced and the new value\n",
    "replace_dict = {\n",
    "    'clinvar: UNK': 'VUS',\n",
    "    'clinvar: other': 'VUS',\n",
    "    'clinvar: Conflicting_interpretations_of_pathogenicity': 'VUS',\n",
    "    'clinvar: Conflicting_interpretations_of_pathogenicity,_other': 'VUS',\n",
    "    'clinvar: Conflicting_interpretations_of_pathogenicity,_association': 'VUS',\n",
    "    'clinvar: Conflicting_interpretations_of_pathogenicity,_risk_factor': 'VUS',\n",
    "    'clinvar: Conflicting_interpretations_of_pathogenicity,_other,_risk_factor': 'VUS',\n",
    "    'clinvar: Conflicting_interpretations_of_pathogenicity,_association,_other,_risk_factor': 'VUS',\n",
    "    'clinvar: Uncertain_significance|drug_response': 'VUS',\n",
    "    'clinvar: Uncertain_significance': 'VUS',\n",
    "    'clinvar: Pathogenic/Likely_pathogenic': 'Pathogenic',\n",
    "    'clinvar: Pathogenic,_other,_risk_factor': 'Pathogenic',\n",
    "    'clinvar: Pathogenic/Likely_pathogenic,_risk_factor': 'Pathogenic',\n",
    "    'clinvar: Pathogenic,_risk_factor': 'Pathogenic',\n",
    "    'clinvar: Pathogenic/Likely_pathogenic,_other': 'Pathogenic',\n",
    "    'clinvar: drug_response': 'Pathogenic',\n",
    "    'clinvar: Pathogenic': 'Pathogenic',\n",
    "    'clinvar: Likely_pathogenic': 'Pathogenic',\n",
    "    'clinvar: Likely_pathogenic,_risk_factor': 'Pathogenic',\n",
    "    'clinvar: Pathogenic|drug_response|other': 'Pathogenic',\n",
    "    'clinvar: Pathogenic/Likely_pathogenic|drug_response': 'Pathogenic',\n",
    "    'clinvar: Pathogenic/Likely_pathogenic|other': 'Pathogenic',\n",
    "    'clinvar: Pathogenic/Likely_pathogenic|association|other': 'Pathogenic',\n",
    "    'clinvar: Likely_pathogenic|risk_factor': 'Pathogenic',\n",
    "    'clinvar: Benign': 'Benign',\n",
    "    'clinvar: Likely_benign': 'Benign',\n",
    "    'clinvar: not_provided': 'VUS',\n",
    "    'clinvar: Benign/Likely_benign': 'Benign',\n",
    "}\n",
    "\n",
    "# Replace the values in the 'clinvar' column\n",
    "merged_df['clinvar:_Clinvar'] = merged_df['clinvar:_Clinvar'].replace(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8307ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip extra spaces from the values in the 'clinvar: Clinvar' column\n",
    "merged_df['InterVar_automated'] = merged_df['InterVar_automated'].astype(str).str.strip()\n",
    "\n",
    "# Rearrangement of clinvar column\n",
    "# values to be replaced and the new value\n",
    "replace_dict = {\n",
    "'PATHOGENIC' : 'Pathogenic',\n",
    "'LIKELY_PATHOGENIC': 'Pathogenic',\n",
    "'LIKELY_BENIGN': 'Benign',\n",
    "'Likely_benign': 'Benign',\n",
    "'BENIGN': 'Benign',\n",
    "'UNCERTAIN_SIGNIFICANCE': 'VUS',\n",
    "'Uncertain_significance': 'VUS',\n",
    "'.': 'VUS'\n",
    "\n",
    "}\n",
    "\n",
    "# Replace the values in the 'clinvar' column\n",
    "merged_df['InterVar_automated'] = merged_df['InterVar_automated'].replace(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0c74d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Replace spaces in column names with underscores\n",
    "merged_df.columns = merged_df.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecccd11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sift = pd.get_dummies(merged_df.SIFT_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b4fee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_sift.rename(columns={\n",
    "    'D': 'SIFT_Deleterious',\n",
    "    'T': 'SIFT_Tolerated',\n",
    "    '.': 'SIFT_Unknown'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa2c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([merged_df,df_sift],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00a40dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polyphen = pd.get_dummies(merged_df.Polyphen2_HVAR_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c132fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_polyphen.rename(columns={\n",
    "    'D': 'Polyphen_Damaging',\n",
    "    'B': 'Polyphen_Benign',\n",
    "    '.': 'Polyphen_Unknown',\n",
    "    'P': 'Polyphen_Possibly_damaging'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6608a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([merged,df_polyphen],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3184833",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MT = pd.get_dummies(merged.MutationTaster_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0b48f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_MT.rename(columns={\n",
    "    'A': 'MutationTaster_Known_deleterious',\n",
    "    'N': 'MutationTaster_Probably_harmless',\n",
    "    'P': 'MutationTaster_Knowntobeharmless',\n",
    "    '.': 'MutationTaster_Unknown',\n",
    "    'D': 'MutationTaster_Probably_deleterious'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0ed77d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([merged,df_MT],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "977b896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final =  merged.drop(['SIFT_pred','Polyphen2_HVAR_pred','MutationTaster_pred'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70af4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin = pd.get_dummies(final['clinvar:_Clinvar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e4748b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "clin.rename(columns={\n",
    "    'Benign': 'Clinvar_Benign',\n",
    "    'Pathogenic': 'Clinvar_Pathogenic',\n",
    "    'VUS': 'Clinvar_VUS'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a58040a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged1 = pd.concat([final,clin],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e080df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervar = pd.get_dummies(merged1['InterVar_automated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9145fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "intervar.rename(columns={\n",
    "    'Benign': 'Intervar_Benign',\n",
    "    'Pathogenic': 'Intervar_Pathogenic',\n",
    "    'VUS': 'Intervar_VUS'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c80dce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged1 = pd.concat([merged1,intervar],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32ea8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 =  merged1.drop(['clinvar:_Clinvar', 'InterVar_automated'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdcaaae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_sub_pref = pd.get_dummies(final1['Substitution_Pref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a07f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rl_sub_pref.rename(columns={\n",
    "    'All protein types': 'Substitution_Pref_All_protein_types',\n",
    "    #'Extracellular proteins': 'RL_Extracellular_proteins',\n",
    "    #'Intracellular proteins': 'RL_Intracellular_proteins',\n",
    "    #'Membrane proteins': 'RL_Membrane_proteins',\n",
    "    'NA': 'Substitution_Pref_NA'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e1bf9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_RL1 = pd.concat([final1,rl_sub_pref],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2573971",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_RL1 =  merged_RL1.drop(['Substitution_Type', 'Score', 'Substitution_Pref'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5ce7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_sub_nature = pd.get_dummies(merged_RL1['Substitution_Nature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0528b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rl_sub_nature.rename(columns={\n",
    "    'Disfavoured': 'Substitution_Nature_Disfavoured',\n",
    "    'Favoured': 'Substitution_Nature_Favoured',\n",
    "    'NA': 'Substitution_Nature_NA',\n",
    "    'Neutral': 'Substitution_Nature_Neutral'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3584e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_RL2 = pd.concat([merged_RL1,rl_sub_nature],axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "469606cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_RL2 =  merged_RL2.drop(['Substitution_Nature'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe1b6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the 'ExonicFunc.ensGene' column to lowercase\n",
    "merged_RL2['ExonicFunc.ensGene'] = merged_RL2['ExonicFunc.ensGene'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07e8ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: One-hot encoding for 'Ref.Gene' if it is a categorical column\n",
    "merged_RL2 = pd.get_dummies(merged_RL2, columns=['Func.ensGene', 'ExonicFunc.ensGene','Ref.Gene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df642a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_RL2 = merged_RL2.drop(['CHROM_x','POS_x', 'End_x', 'REF_x','ALT_x','AA_Change'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac2e8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: One-hot encoding for 'Ref.Gene' if it is a categorical column\n",
    "merged_RL2 = pd.get_dummies(merged_RL2, columns=['Main_AA', 'Sub_AA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63cad4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean columns to integer (0, 1)\n",
    "bool_cols = merged_RL2.select_dtypes(include='bool').columns\n",
    "merged_RL2[bool_cols] = merged_RL2[bool_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "020c9001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: ['MutationTaster_Knowntobeharmless', 'Func.ensGene_UTR3', 'Func.ensGene_UTR5', 'Func.ensGene_exonic;splicing', 'Func.ensGene_intronic', 'Func.ensGene_ncRNA_intronic', 'ExonicFunc.ensGene_nonframeshift substitution', 'ExonicFunc.ensGene_stoploss', 'ExonicFunc.ensGene_synonymous snv', 'Ref.Gene_ABL1', 'Ref.Gene_BTBD1', 'Ref.Gene_CD74', 'Ref.Gene_CDK4', 'Ref.Gene_CDK6', 'Ref.Gene_EML4', 'Ref.Gene_EPCAM', 'Ref.Gene_ERBB3', 'Ref.Gene_ETV6', 'Ref.Gene_EZH2', 'Ref.Gene_IDH1', 'Ref.Gene_MAP2K1', 'Ref.Gene_NRAS', 'Ref.Gene_PDGFRB', 'Ref.Gene_QKI', 'Ref.Gene_RAD51C', 'Ref.Gene_RAD51D', 'Ref.Gene_SDC4', 'Ref.Gene_SMAD4', 'Ref.Gene_SRY', 'Ref.Gene_TRIM24']\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_RL2[col] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\119329006.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.898312\n",
      "1          Benign               0.992047\n",
      "2          Benign               0.995804\n",
      "3          Benign               0.468812\n",
      "4          Benign               0.999430\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_LR = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/LR_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_LR.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = model_LR.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are one-hot encoded, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the predicted class (highest probability)\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # pred is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_LR.xlsx\", index=False) \n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78547e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\3480555622.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\3480555622.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.997963\n",
      "1          Benign               0.980715\n",
      "2          Benign               0.990657\n",
      "3             VUS               0.848804\n",
      "4          Benign               0.999646\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_LR_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/LR_model_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_LR_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = model_LR_smote.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are one-hot encoded, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the predicted class (highest probability)\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # pred is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_LR_smote.xlsx\", index=False) \n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b327da06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n",
      "Predictions shape: (307, 3)\n",
      "Predictions sample: [[0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\3460432632.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\3460432632.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic                    1.0\n",
      "1          Benign                    1.0\n",
      "2          Benign                    1.0\n",
      "3             VUS                    0.8\n",
      "4          Benign                    1.0\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_KNN = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/KNN_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_KNN.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = model_KNN.predict(test_data)\n",
    "\n",
    "# Check the shape of predictions to see if it's a one-hot encoded array\n",
    "print(\"Predictions shape:\", predictions.shape)  # Debugging line\n",
    "print(\"Predictions sample:\", predictions[:5])  # Debugging line\n",
    "\n",
    "# Step 3: If predictions are one-hot encoded, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the predicted class (highest probability)\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_KNN.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffad988e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n",
      "Predictions shape: (307, 3)\n",
      "Predictions sample: [[0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\1404887355.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\1404887355.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic                    1.0\n",
      "1          Benign                    1.0\n",
      "2          Benign                    1.0\n",
      "3             VUS                    1.0\n",
      "4          Benign                    1.0\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_KNN_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/KNN_model_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_KNN_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = model_KNN_smote.predict(test_data)\n",
    "\n",
    "# Check the shape of predictions to see if it's a one-hot encoded array\n",
    "print(\"Predictions shape:\", predictions.shape)  # Debugging line\n",
    "print(\"Predictions sample:\", predictions[:5])  # Debugging line\n",
    "\n",
    "# Step 3: If predictions are one-hot encoded, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the predicted class (highest probability)\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_KNN_smote_prob.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb0bfcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n",
      "Predictions shape: (307, 3)\n",
      "Predictions sample: [[0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\1545393587.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\1545393587.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic                    1.0\n",
      "1          Benign                    1.0\n",
      "2          Benign                    1.0\n",
      "3          Benign                    1.0\n",
      "4          Benign                    1.0\n"
     ]
    }
   ],
   "source": [
    "model_DT = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/DT_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_DT.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (class labels directly)\n",
    "predictions = model_DT.predict(test_data)\n",
    "\n",
    "# Check the shape of predictions to see if it's a one-hot encoded array\n",
    "print(\"Predictions shape:\", predictions.shape)  # Debugging line\n",
    "print(\"Predictions sample:\", predictions[:5])  # Debugging line\n",
    "\n",
    "# Step 3: If predictions are one-hot encoded, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the predicted class (highest probability)\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_DT.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ea95372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n",
      "Predictions shape: (307, 3)\n",
      "Predictions sample: [[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [1 0 1]\n",
      " [1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\3138970197.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\3138970197.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic                    1.0\n",
      "1          Benign                    1.0\n",
      "2          Benign                    0.0\n",
      "3          Benign                    1.0\n",
      "4          Benign                    1.0\n"
     ]
    }
   ],
   "source": [
    "model_DT_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/DT_model_with_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_DT_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (class labels directly)\n",
    "predictions = model_DT_smote.predict(test_data)\n",
    "\n",
    "# Check the shape of predictions to see if it's a one-hot encoded array\n",
    "print(\"Predictions shape:\", predictions.shape)  # Debugging line\n",
    "print(\"Predictions sample:\", predictions[:5])  # Debugging line\n",
    "\n",
    "# Step 3: If predictions are one-hot encoded, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the predicted class (highest probability)\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_DT_smote_prob.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42e83b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\3601103453.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\3601103453.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic                 1.0000\n",
      "1          Benign                 1.0000\n",
      "2          Benign                 0.9100\n",
      "3             VUS                 0.7415\n",
      "4          Benign                 1.0000\n"
     ]
    }
   ],
   "source": [
    "model_RF = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/RF_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_RF.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (class labels directly)\n",
    "predictions = model_RF.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a one-hot encoded format, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_RF.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3572c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\2863143396.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\2863143396.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic                   1.00\n",
      "1          Benign                   1.00\n",
      "2          Benign                   0.89\n",
      "3             VUS                   0.99\n",
      "4          Benign                   1.00\n"
     ]
    }
   ],
   "source": [
    "model_RF_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/RF_model_with_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = model_RF_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (class labels directly)\n",
    "predictions = model_RF_smote.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a one-hot encoded format, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_RF_smote_prob.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99622248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\3549158159.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\3549158159.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.981742\n",
      "1          Benign               0.993754\n",
      "2          Benign               0.995632\n",
      "3             VUS               0.829984\n",
      "4          Benign               0.999189\n"
     ]
    }
   ],
   "source": [
    "XGB_model = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/XGB_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = XGB_model.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (class labels directly)\n",
    "predictions = XGB_model.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a one-hot encoded format, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_XGB.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "174456b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\1291249262.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\1291249262.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.996154\n",
      "1          Benign               0.960283\n",
      "2          Benign               0.977577\n",
      "3             VUS               0.894453\n",
      "4          Benign               0.991338\n"
     ]
    }
   ],
   "source": [
    "XGB_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/XGB_with_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = XGB_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (class labels directly)\n",
    "predictions = XGB_smote.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a one-hot encoded format, use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]  # `pred` is the predicted class index\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_XGB_smote_prob.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e4c13fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\609978689.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\609978689.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.984495\n",
      "1          Benign               0.994472\n",
      "2          Benign               0.997374\n",
      "3             VUS               0.874930\n",
      "4          Benign               0.996667\n"
     ]
    }
   ],
   "source": [
    "LGB_model = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/LGB_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = LGB_model.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = LGB_model.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a 2D array (one-hot encoded), use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_LGB.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d757737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\3102011765.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\3102011765.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.994412\n",
      "1          Benign               0.943648\n",
      "2          Benign               0.982893\n",
      "3             VUS               0.810766\n",
      "4          Benign               0.994680\n"
     ]
    }
   ],
   "source": [
    "LGB_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/LGB_with_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained model\n",
    "pred_probabilities = LGB_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with highest probability)\n",
    "predictions = LGB_smote.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a 2D array (one-hot encoded), use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_LGB_smote_prob.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d78587b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\4291035992.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\4291035992.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.984461\n",
      "1          Benign               0.992719\n",
      "2          Benign               0.977613\n",
      "3             VUS               0.693609\n",
      "4          Benign               0.998416\n"
     ]
    }
   ],
   "source": [
    "CB_model = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/CB_model.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained CatBoost model\n",
    "pred_probabilities = CB_model.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with the highest probability)\n",
    "predictions = CB_model.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a 2D array (one-hot encoded), use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_CB.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e803b207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Extra columns: ['Sub_AA_ALa']\n",
      "Columns in test data after alignment: Index(['SIFT_Unknown', 'SIFT_Deleterious', 'SIFT_Tolerated',\n",
      "       'Polyphen_Unknown', 'Polyphen_Benign', 'Polyphen_Damaging',\n",
      "       'Polyphen_Possibly_damaging', 'MutationTaster_Unknown',\n",
      "       'MutationTaster_Known_deleterious',\n",
      "       'MutationTaster_Probably_deleterious',\n",
      "       ...\n",
      "       'Sub_AA_Unknown', 'Sub_AA_Val', 'Sub_AA_delins_Unknown',\n",
      "       'Sub_AA_fs_Unknown', 'Substitution_Pref_All_protein_types',\n",
      "       'Substitution_Pref_NA', 'Substitution_Nature_Disfavoured',\n",
      "       'Substitution_Nature_Favoured', 'Substitution_Nature_NA',\n",
      "       'Substitution_Nature_Neutral'],\n",
      "      dtype='object', length=165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\1728899879.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Class'] = predicted_classes\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14288\\1728899879.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data['Predicted_Probability'] = predicted_probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Class  Predicted_Probability\n",
      "0      Pathogenic               0.999821\n",
      "1          Benign               0.948294\n",
      "2          Benign               0.983462\n",
      "3             VUS               0.880916\n",
      "4          Benign               0.998294\n"
     ]
    }
   ],
   "source": [
    "CB_smote = joblib.load('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/CB_with_smote.pkl')\n",
    "\n",
    "import json\n",
    "# Load the trained columns\n",
    "with open('C:/Users/HP/Downloads/TID-Work/saved_models/T1/Target-2/trained_columns.json', 'r') as f:\n",
    "    trained_columns = json.load(f)\n",
    "\n",
    "# Check missing and extra columns\n",
    "missing_columns = [col for col in trained_columns if col not in merged_RL2.columns]\n",
    "extra_columns = [col for col in merged_RL2.columns if col not in trained_columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)\n",
    "print(\"Extra columns:\", extra_columns)\n",
    "\n",
    "# Add missing columns with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    merged_RL2[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "test_data = merged_RL2.drop(columns=extra_columns)\n",
    "\n",
    "# Reorder columns to match trained model's feature order\n",
    "test_data = test_data[trained_columns]\n",
    "\n",
    "# Verify columns match before prediction\n",
    "print(\"Columns in test data after alignment:\", test_data.columns)\n",
    "\n",
    "# Step 1: Get prediction probabilities using the trained CatBoost model\n",
    "pred_probabilities = CB_smote.predict_proba(test_data)\n",
    "\n",
    "# Step 2: Make predictions for the classes (get the class index with the highest probability)\n",
    "predictions = CB_smote.predict(test_data)\n",
    "\n",
    "# Step 3: If predictions are in a 2D array (one-hot encoded), use np.argmax() to get the class index\n",
    "if predictions.ndim > 1:\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each sample\n",
    "\n",
    "# Step 4: Get the probabilities for the predicted class\n",
    "predicted_probabilities = [pred_probabilities[i, int(pred)] for i, pred in enumerate(predictions)]\n",
    "\n",
    "# Step 5: Map predictions (numeric) to class labels\n",
    "class_labels = {0: 'Benign', 1: 'Pathogenic', 2: 'VUS'}\n",
    "predicted_classes = [class_labels[int(pred)] for pred in predictions]\n",
    "\n",
    "# Step 6: Add predictions and probabilities to the test data\n",
    "test_data['Predicted_Class'] = predicted_classes\n",
    "test_data['Predicted_Probability'] = predicted_probabilities\n",
    "\n",
    "# Step 7: Output the results to Excel\n",
    "test_data.to_excel(\"Predicted_Classes_for_Test_Data_CAT_smote_prob.xlsx\", index=False)\n",
    "\n",
    "# Optionally, display the predictions with probabilities\n",
    "print(test_data[['Predicted_Class', 'Predicted_Probability']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d510812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
